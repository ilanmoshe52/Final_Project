{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Q-xxqAZiAi7h",
        "trusted": true
      },
      "source": [
        "**Sentiment Analysis of IMDB Movie Reviews**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a2IUbS24Ai7j"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TpZXCsBEnh-",
        "outputId": "442d99b8-49f8-412a-ba55-b600232b41bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Requirement already satisfied: joblib in c:\\users\\97252\\desktop\\pytorch-project\\ilan\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\97252\\desktop\\pytorch-project\\ilan\\lib\\site-packages (from nltk) (2023.5.5)\n",
            "Requirement already satisfied: tqdm in c:\\users\\97252\\desktop\\pytorch-project\\ilan\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\97252\\desktop\\pytorch-project\\ilan\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Installing collected packages: click, nltk\n",
            "Successfully installed click-8.1.3 nltk-3.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet2022 to\n",
            "[nltk_data]     C:\\Users\\97252\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\97252\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet2022') #wordnet2022\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51",
        "id": "CXj1PDEZAi7k"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "X2j3sV38Ai7k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "#print(os.listdir(\"../input\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "be1b642cce343f7a8f68f8c91f7c50372cdf4381",
        "id": "qGC7F_jPAi7m"
      },
      "source": [
        "**Import the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "lNvJEGlIAi7m",
        "outputId": "eb0bb6fa-2409-410e-b803-6c9c614d7b99",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>For a movie that gets no respect there sure ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You probably all already know this by now, but...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I saw the movie with two grown children. Altho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>You're using the IMDb.  You've given some heft...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This was a good film with a powerful message o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Made after QUARTET was, TRIO continued the qua...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>For a mature man, to admit that he shed a tear...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  For a movie that gets no respect there sure ar...          0\n",
              "1  Bizarre horror movie filled with famous faces ...          0\n",
              "2  A solid, if unremarkable film. Matthau, as Ein...          0\n",
              "3  It's a strange feeling to sit alone in a theat...          0\n",
              "4  You probably all already know this by now, but...          0\n",
              "5  I saw the movie with two grown children. Altho...          0\n",
              "6  You're using the IMDb.  You've given some heft...          0\n",
              "7  This was a good film with a powerful message o...          0\n",
              "8  Made after QUARTET was, TRIO continued the qua...          0\n",
              "9  For a mature man, to admit that he shed a tear...          0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#importing the training data\n",
        "imdb_data=pd.read_csv('C:/Users/97252/Desktop/pytorch-project/train.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295",
        "id": "WxZ79x68Ai7n"
      },
      "source": [
        "**Exploratery data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "3MV113mTAi7n",
        "outputId": "ff487720-d859-43d2-85a2-31bbf777a494",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>25000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment\n",
              "count  25000.00000\n",
              "mean       0.50000\n",
              "std        0.50001\n",
              "min        0.00000\n",
              "25%        0.00000\n",
              "50%        0.50000\n",
              "75%        1.00000\n",
              "max        1.00000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Summary of the dataset\n",
        "imdb_data.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d",
        "id": "d5LuAOvvAi7n"
      },
      "source": [
        "**Sentiment count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q07oDMK8Ai7n",
        "outputId": "f7b2d6a6-8ed8-4fe1-9b67-c378db15688e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "0    12500\n",
              "1    12500\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sentiment count\n",
        "imdb_data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ilKQiecCyS9",
        "outputId": "a56acf2e-558e-416a-8d95-32db3e5eaad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000,) (20000,)\n",
            "(5000,) (5000,)\n"
          ]
        }
      ],
      "source": [
        "X = imdb_data['text']\n",
        "y = imdb_data['sentiment']\n",
        "train_reviews, test_reviews, train_sentiments, test_sentiments =  train_test_split( X,y ,\n",
        "                                                                                    random_state=104, \n",
        "                                                                                    test_size=0.2, \n",
        "                                                                                    shuffle=True)\n",
        "\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyl2iwheAi7n"
      },
      "source": [
        "We can see that the dataset is balanced."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f61964573faababe1f7897b77d32815a24954d2f",
        "id": "C5bYjmCQAi7o"
      },
      "source": [
        "**Spliting the training dataset**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac",
        "id": "Iw3cT6tuAi7p"
      },
      "source": [
        "**Text normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_uuid": "f000c43d91f68f6668539f089c6a54c5ce3bd819",
        "id": "juvUybKTAi7p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "328b6e5977da3e055ad4b2e11a31e5e12ccf3b16",
        "id": "nCfjEBV7Ai7p"
      },
      "source": [
        "**Removing html strips and noise text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "_uuid": "6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a",
        "id": "Lgfhi4DbAi7p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['text']=imdb_data['text'].apply(denoise_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "88117b74761d1047924d6d70f76642faa0e706ac",
        "id": "WNEynQM6Ai7p"
      },
      "source": [
        "**Removing special characters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "_uuid": "219da72b025121fd98081df50ae0fcaace10cc9d",
        "id": "EyRLem-dAi7q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['text']=imdb_data['text'].apply(remove_special_characters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3b66eeabd5b7b8c251f8b8ddf331140a64bcd514",
        "id": "HjQldR4-Ai7q"
      },
      "source": [
        "**Text stemming\n",
        "**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "_uuid": "2295f2946e0ab74c220ad538d0e7adc04d23f697",
        "id": "cWUFfp2iAi7q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['text']=imdb_data['text'].apply(simple_stemmer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e83107e4a281d84d7ae42b4e2c8d81b7ece438e4",
        "id": "mb_otO02Ai7q"
      },
      "source": [
        "**Removing stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_uuid": "5dbff82b4d2d188d8777b273a75d8ac714d38885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3olCfeT1Ai7q",
        "outputId": "7e002393-7825-4b3d-85ce-3ea4b3ebaacf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'other', 'hers', 'isn', 'their', 'which', 'doesn', \"you'd\", 'the', 'who', 'them', 'being', \"it's\", 'only', 'am', 'y', 'while', 'than', 'needn', 'ours', \"doesn't\", 'from', 'can', 'both', \"didn't\", 'now', \"needn't\", 'shouldn', \"weren't\", 'over', \"hadn't\", 'such', 'so', 'yourselves', 'then', 'should', 'd', 'there', 'its', 'where', 'further', 'it', 'he', 'you', 'will', \"isn't\", 'off', 'or', 'more', 'above', \"mightn't\", 'your', 'after', 'hasn', 'between', 'no', \"don't\", \"should've\", 'why', \"wasn't\", 'ourselves', 'an', \"haven't\", 'to', 'with', 'couldn', 'i', 'll', 'they', 'a', 'didn', 'once', 'does', 'any', 'had', 'below', 'has', 'and', 'o', \"wouldn't\", 'myself', 'themselves', 'up', 'under', 'out', 'me', 'yourself', 'what', 'be', 'been', 'each', \"couldn't\", 'my', \"you're\", \"mustn't\", 'hadn', 'that', \"she's\", 'are', 'doing', 'shan', \"aren't\", 'against', 'yours', 're', 'herself', 'did', 'on', 'mightn', 'these', \"hasn't\", 'by', 'him', 'do', 'because', 'very', \"shouldn't\", 'as', 'again', 'theirs', 'our', 'won', 'about', 'those', 'weren', 'how', 'for', 'into', 'if', 'at', 'but', 'this', 'her', 'she', 'nor', 'ma', 'haven', 'himself', 'ain', \"won't\", 'were', 'have', 'we', 'some', 's', \"shan't\", \"you've\", 'few', 'having', 'during', 'down', 'same', 'just', 'when', \"you'll\", 'most', 'all', 'mustn', 'in', 'until', \"that'll\", 'is', 'through', 'before', 'wasn', 'itself', 'not', 'too', 't', 'here', 'of', 've', 'his', 'wouldn', 'whom', 'don', 'was', 'aren', 'own', 'm'}\n"
          ]
        }
      ],
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "imdb_data['text']=imdb_data['text'].apply(remove_stopwords)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b35e7499291173119ed42287deac6f0cd96516e1",
        "id": "ECx0Am8sAi7r"
      },
      "source": [
        "**Normalized train reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "jdzNslm-Ai7r",
        "outputId": "3b21938a-23da-4380-c892-2cc51ed8df9c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_train_reviews= train_reviews\n",
        "norm_train_reviews[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d69462bb209a66cff86376dc8481d0c0140d894d",
        "id": "BOMAa86ZAi7r"
      },
      "source": [
        "**Normalized test reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "c5d0d38bd9976150367e9d75f3b933774c96a1ab",
        "id": "ehD3xRstAi7r",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "norm_test_reviews=test_reviews"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1c2a872ffcb6b8076fdbbba641af12081b6022ef",
        "id": "Y67nzZEqAi7s"
      },
      "source": [
        "**Bags of words model **\n",
        "\n",
        "It is used to convert text documents to numerical vectors or bag of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "_uuid": "35cf9dcefb40b2dc520c5b0d559695324c46cc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRUb5KX_Ai7s",
        "outputId": "74e89b45-104e-4593-db4a-368fdd97faaf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOW_cv_train: (20000, 3578773)\n",
            "BOW_cv_test: (5000, 3578773)\n"
          ]
        }
      ],
      "source": [
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)\n",
        "#vocab=cv.get_feature_names()-toget feature names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d",
        "id": "0tceMaN3Ai7s"
      },
      "source": [
        "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
        "\n",
        "It is used to convert text documents to  matrix of  tfidf features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTQCzjRXAi7s",
        "outputId": "a249d905-8360-4e14-af8a-78001d45de74",
        "scrolled": false,
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tfidf_train: (20000, 3578773)\n",
            "Tfidf_test: (5000, 3578773)\n"
          ]
        }
      ],
      "source": [
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "803e92b25faa738b10928a91de72d177d8dddf85",
        "id": "R8IungAYAi7t"
      },
      "source": [
        "**Labeling the sentiment text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_uuid": "60f5d496ce4109d1cdbf08f4284d4d26efd93922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07q8S36cAi7t",
        "outputId": "7b62ab1d-a921-4ef8-8307-504caa19a4c4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 1)\n"
          ]
        }
      ],
      "source": [
        "#labeling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "#transformed sentiment data\n",
        "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
        "print(sentiment_data.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "21a80c94fb42e14391c627710c5d796c40aa7dde",
        "id": "emVKsB0UAi7t"
      },
      "source": [
        "**Split the sentiment tdata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "ca1e4cc917265ac98a72c37cffe57f27e9897408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFbCpsRzAi7t",
        "outputId": "5f23cb97-ce80-4639-8eef-fdd1316540aa",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22081    1\n",
            "12045    0\n",
            "8173     0\n",
            "18290    1\n",
            "21564    1\n",
            "        ..\n",
            "21631    1\n",
            "6310     0\n",
            "17113    1\n",
            "22209    1\n",
            "8261     0\n",
            "Name: sentiment, Length: 20000, dtype: int64\n",
            "598      0\n",
            "10039    0\n",
            "21590    1\n",
            "13701    1\n",
            "1823     0\n",
            "        ..\n",
            "13401    1\n",
            "13771    1\n",
            "10164    0\n",
            "22505    1\n",
            "4305     0\n",
            "Name: sentiment, Length: 5000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Spliting the sentiment data\n",
        "#train_sentiments=sentiment_data[:40000]\n",
        "#test_sentiments=sentiment_data[40000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TijN3_9xAi7t"
      },
      "source": [
        "**Modelling the dataset**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d5e45fdc9d062a5b9b9dd665ffe732776e196953",
        "id": "KauFa1W8Ai7t"
      },
      "source": [
        "Let us build logistic regression model for both bag of words and tfidf features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "_uuid": "142d007421900550079a12ae8655bcae678ebaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qrwv66OAi7t",
        "outputId": "9c613da4-2dae-4d82-d30a-841ed4c6b95f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
          ]
        }
      ],
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
        "print(lr_bow)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
        "print(lr_tfidf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "07eb6d52eb32469e3be82e90af636d598a7b7c27",
        "id": "_TbpNmq6Ai7u"
      },
      "source": [
        "**Logistic regression model performane on test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "_uuid": "52ad86935b76117f97b79e6672a3ba12352b9461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAdQu7ZxAi7u",
        "outputId": "bd097629-ea39-4ac7-cdad-abafa1336f69",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 1 0 0]\n",
            "[0 0 1 ... 1 0 0]\n"
          ]
        }
      ],
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "##Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0ltn7GhOAi7u"
      },
      "source": [
        "**Accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAJtw2-YAi7u",
        "outputId": "e55d8dfa-ad62-4955-f8b1-01645a672dac",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_bow_score : 0.775\n",
            "lr_tfidf_score : 0.7676\n"
          ]
        }
      ],
      "source": [
        "#Accuracy score for bag of words\n",
        "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ac2ec8353acb5e0f548e1e4a590fbe6f34f4a686",
        "id": "6Qe2w2RBAi7u"
      },
      "source": [
        "**Print the classification report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "_uuid": "f89c7e7a6136d08790ffbf6bc4d0d05455f8555a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCTb6TuGAi7u",
        "outputId": "ae3ac25f-c6ec-4a29-a703-b2029f0871a4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.78      0.77      0.78      2533\n",
            "    Negative       0.77      0.78      0.77      2467\n",
            "\n",
            "    accuracy                           0.78      5000\n",
            "   macro avg       0.78      0.78      0.77      5000\n",
            "weighted avg       0.78      0.78      0.78      5000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.80      0.72      0.76      2533\n",
            "    Negative       0.74      0.82      0.78      2467\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.77      0.77      0.77      5000\n",
            "weighted avg       0.77      0.77      0.77      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Classification report for bag of words \n",
        "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "#Classification report for tfidf features\n",
        "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(lr_tfidf_report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0d2e5ddcd69ff0fb52f05f17fc74a86e1b5e5b61",
        "id": "X5ybGqhEAi7u"
      },
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "_uuid": "a36c058e834938559b7202f2142e61423a613b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP_TUFnFAi7u",
        "outputId": "d1e72f21-295d-42e0-ca38-580a78e8bab0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1935  532]\n",
            " [ 593 1940]]\n",
            "[[2026  441]\n",
            " [ 721 1812]]\n"
          ]
        }
      ],
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter q to stop\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Expected 2D array, got scalar array instead:\narray=this is a great movie.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m   prediction  \u001b[39m=\u001b[39mlr\u001b[39m.\u001b[39;49mpredict(user_input)\n\u001b[0;32m      8\u001b[0m   \u001b[39mprint\u001b[39m(prediction)\n",
            "File \u001b[1;32mc:\\Users\\97252\\Desktop\\pytorch-project\\ilan\\lib\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\97252\\Desktop\\pytorch-project\\ilan\\lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
            "File \u001b[1;32mc:\\Users\\97252\\Desktop\\pytorch-project\\ilan\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32mc:\\Users\\97252\\Desktop\\pytorch-project\\ilan\\lib\\site-packages\\sklearn\\utils\\validation.py:894\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    892\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 894\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    895\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    896\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    897\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    898\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    899\u001b[0m         )\n\u001b[0;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
            "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=this is a great movie.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ],
      "source": [
        "print('Enter q to stop')\n",
        "while(True):\n",
        "  user_input = input(\"Enter sentance to be eveluated:\")\n",
        "  if user_input == 'q':\n",
        "    break\n",
        "  else:\n",
        "    prediction  =lr.predict(user_input)\n",
        "    print(prediction)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x3578773 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_test_reviews[0][0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XtF2_rX1Ai7x"
      },
      "source": [
        "**Conclusion:**\n",
        "* We can observed that both logistic regression and multinomial naive bayes model performing well compared to linear support vector  machines.\n",
        "* Still we can improve the accuracy of the models by preprocessing data and by using lexicon models like Textblob."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
